{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4939c1-25a6-4f3a-b318-3f16d49d3377",
   "metadata": {},
   "source": [
    "## 3.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d370e0-43dc-4bf4-b9b7-e4d570e61b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 3.3 — BASELINE BENCHMARK (Pre-Fine-Tuning)\n",
    "# Model: google/flan-t5-base\n",
    "# What this cell does:\n",
    "#   1) Load model/tokenizer on CPU/GPU\n",
    "#   2) Build/parse a test set of ≥10 Q/A (from your 3.1 dataset if possible; else fallback)\n",
    "#   3) Define normalization & matching utilities (numeric tolerance + text)\n",
    "#   4) Run baseline inference (answer, proxy-confidence, latency)\n",
    "#   5) Compute accuracy & summary stats; save CSV artifacts\n",
    "# ============================================\n",
    "\n",
    "# -------- 1) Load model & tokenizer --------\n",
    "import re, math, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"  # decided in 3.2\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"[INFO] Loaded {MODEL_NAME} on {device}\")\n",
    "\n",
    "# -------- 2) Build a test set (≥10 items) --------\n",
    "# Tries to parse your existing HuggingFace `dataset` from 3.1 which stores \"text\": \"question: ... answer: ...\"\n",
    "# If not available/parsing fails, falls back to placeholders (edit with your real questions/answers).\n",
    "\n",
    "def parse_qa_from_text(s: str):\n",
    "    \"\"\"expects 'question: ... answer: ...' format (case-insensitive)\"\"\"\n",
    "    m = re.match(r\"\\s*question\\s*:\\s*(.*?)\\s*answer\\s*:\\s*(.*)\\s*$\", str(s).strip(), flags=re.I|re.S)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    q, a = m.group(1).strip(), m.group(2).strip()\n",
    "    return q, a\n",
    "\n",
    "TEST_QA = []\n",
    "try:\n",
    "    # if 'dataset' (from Section 3.1) exists, try to parse a sample\n",
    "    _df = pd.DataFrame(dataset)  # relies on your previous cell's variable\n",
    "    # random sample up to 12 to leave a little margin\n",
    "    sample_df = _df.sample(min(12, len(_df)), random_state=42)\n",
    "    for _, row in sample_df.iterrows():\n",
    "        q, a = parse_qa_from_text(row[\"text\"])\n",
    "        if q and a:\n",
    "            TEST_QA.append({\"question\": q, \"ground_truth\": a})\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] Could not parse from your 3.1 dataset automatically: {e}\")\n",
    "\n",
    "if len(TEST_QA) < 10:\n",
    "    print(\"[NOTICE] Using placeholder test QA — replace with your real 10+ questions.\")\n",
    "    TEST_QA.extend([\n",
    "        {\"question\": \"Total liabilities of 2023?\", \"ground_truth\": \"$ 10,002 million.\"},\n",
    "        {\"question\": \"What were total revenues in fiscal 2024?\", \"ground_truth\": \"$ 16,052 million.\"},\n",
    "        {\"question\": \"Net cash from operating activities in fiscal 2024?\", \"ground_truth\": \"$ 454 million.\"},\n",
    "        {\"question\": \"Adjusted free cash flow in fiscal 2024?\", \"ground_truth\": \"$ 291 million.\"},\n",
    "        {\"question\": \"What was the net loss in fiscal 2024?\", \"ground_truth\": \"$ 340 million loss.\"},\n",
    "        {\"question\": \"Year-over-year revenue change in fiscal 2024 vs 2023?\", \"ground_truth\": \"6% decline\"},\n",
    "        {\"question\": \"Cash and cash equivalents at end of fiscal 2024?\", \"ground_truth\": \"$ 1,554 million.\"},\n",
    "        {\"question\": \"Which geographic segment had highest revenue in fiscal 2024?\", \"ground_truth\": \"Americas segment\"},\n",
    "        {\"question\": \"What strategic partnership was highlighted in fiscal 2024?\", \"ground_truth\": \"Alliances with AWS, Microsoft, and Google Cloud\"},\n",
    "        {\"question\": \"What is the capital of France?\", \"ground_truth\": \"Not in scope\"},\n",
    "    ])\n",
    "\n",
    "# keep exactly 10 for baseline (you can increase if you want)\n",
    "TEST_QA = TEST_QA[:10]\n",
    "print(f\"[INFO] Test set size: {len(TEST_QA)}\")\n",
    "\n",
    "# -------- 3) Normalization & matching utilities --------\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _first_number(s: str):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).replace(\",\", \"\")\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def _is_not_in_scope(s: str) -> bool:\n",
    "    a = _norm_text(s)\n",
    "    phrases = [\n",
    "        \"not in scope\",\"out of scope\",\"no relevant data\",\n",
    "        \"not found in the documents\",\"cannot answer from the provided documents\",\n",
    "        \"data not available\",\"i don't know\",\"i do not know\"\n",
    "    ]\n",
    "    return any(p in a for p in phrases)\n",
    "\n",
    "def compare_answers(gold: str, pred: str, rel_tol: float = 0.02) -> bool:\n",
    "    \"\"\"\n",
    "    Numeric compare if both sides expose a number (within rel_tol, default 2%).\n",
    "    Else relaxed text containment. 'Not in scope' recognized via heuristic.\n",
    "    \"\"\"\n",
    "    gnum = _first_number(gold)\n",
    "    pnum = _first_number(pred)\n",
    "    if gnum is not None and pnum is not None:\n",
    "        if gnum == 0:\n",
    "            return abs(pnum) < 1e-12\n",
    "        return math.isclose(pnum, gnum, rel_tol=rel_tol)\n",
    "    g = _norm_text(gold)\n",
    "    p = _norm_text(pred)\n",
    "    if g == \"not in scope\":\n",
    "        return _is_not_in_scope(p)\n",
    "    return (g in p) or (p in g)\n",
    "\n",
    "# -------- 4) Baseline inference (answer, proxy-confidence, latency) --------\n",
    "@torch.no_grad()\n",
    "def t5_generate_with_conf(question: str, max_new_tokens: int = 64):\n",
    "    \"\"\"\n",
    "    Returns dict(answer, confidence, time_s)\n",
    "    - Confidence is a proxy: mean(max softmax prob) over generated tokens (0..1).\n",
    "    - Deterministic decode (greedy) for stable benchmarking.\n",
    "    \"\"\"\n",
    "    prompt = f\"question: {question}\\nanswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    # decode\n",
    "    text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    # token-wise max prob\n",
    "    max_probs = []\n",
    "    for step_logits in outputs.scores:\n",
    "        probs = F.softmax(step_logits[0], dim=-1)  # (vocab,)\n",
    "        max_probs.append(float(probs.max().item()))\n",
    "    confidence = float(np.mean(max_probs)) if max_probs else None\n",
    "\n",
    "    # extract answer after 'answer:' if present\n",
    "    ans = text\n",
    "    m = re.search(r\"answer\\s*:\\s*(.*)$\", text, flags=re.I|re.S)\n",
    "    if m:\n",
    "        ans = m.group(1).strip()\n",
    "\n",
    "    return {\"answer\": ans, \"confidence\": confidence, \"time_s\": round(dt, 3)}\n",
    "\n",
    "# run evaluation\n",
    "rows = []\n",
    "for i, item in enumerate(TEST_QA, start=1):\n",
    "    q = item[\"question\"]\n",
    "    gold = item[\"ground_truth\"]\n",
    "    out = t5_generate_with_conf(q)\n",
    "    pred = out[\"answer\"]\n",
    "    conf = out[\"confidence\"]\n",
    "    tsec = out[\"time_s\"]\n",
    "    acc = compare_answers(gold, pred)\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"question\": q,\n",
    "        \"ground_truth\": gold,\n",
    "        \"pred_answer\": pred,\n",
    "        \"confidence\": conf if conf is not None else np.nan,\n",
    "        \"time_s\": tsec,\n",
    "        \"correct\": bool(acc),\n",
    "    })\n",
    "\n",
    "baseline_df = pd.DataFrame(rows)\n",
    "display(baseline_df)\n",
    "\n",
    "# -------- 5) Summary stats & save artifacts --------\n",
    "summary = {\n",
    "    \"n\": len(baseline_df),\n",
    "    \"accuracy\": float(baseline_df[\"correct\"].mean()) if len(baseline_df) else 0.0,\n",
    "    \"avg_confidence\": float(baseline_df[\"confidence\"].mean()) if baseline_df[\"confidence\"].notna().any() else None,\n",
    "    \"avg_time_s\": float(baseline_df[\"time_s\"].mean()) if len(baseline_df) else None,\n",
    "}\n",
    "summary_df = pd.DataFrame([summary])\n",
    "display(summary_df)\n",
    "\n",
    "# save CSVs for your report\n",
    "RES_PATH = Path(\"section3_3_baseline_results.csv\")\n",
    "SUM_PATH = Path(\"section3_3_baseline_summary.csv\")\n",
    "baseline_df.to_csv(RES_PATH, index=False)\n",
    "summary_df.to_csv(SUM_PATH, index=False)\n",
    "print(\"[SAVED]\", RES_PATH.resolve())\n",
    "print(\"[SAVED]\", SUM_PATH.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e94d81-d476-4a62-b6ff-62ea70c9cdd2",
   "metadata": {},
   "source": [
    "## Section 4 – Setup & Interfaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0973f4d-69f2-46ff-855a-8db53bc2bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.0: Imports, Data Types, and Placeholders ====\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Any, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, json, re, time\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class QAResult:\n",
    "    \"\"\"\n",
    "    Standard result format used by the evaluators.\n",
    "    \"\"\"\n",
    "    answer: str\n",
    "    confidence: Optional[float]   # 0..1 (None if not available)\n",
    "    metadata: Dict[str, Any]      # e.g., {\"citations\": [...], \"chunks\":[...]}\n",
    "    method_name: str              # e.g., \"RAG\" or \"Fine-Tune\"\n",
    "\n",
    "# === PLACEHOLDERS: wire your real functions here ======================\n",
    "# Replace the bodies of these two functions with calls to your actual\n",
    "# implementations from Sections 2 & 3. Keep the signature.\n",
    "\n",
    "def RAG_ANSWER_FN(question: str) -> QAResult:\n",
    "    # TODO: call your real RAG pipeline here and return QAResult\n",
    "    return QAResult(\n",
    "        answer=\"PLACEHOLDER_RAG_ANSWER\",\n",
    "        confidence=0.0,\n",
    "        metadata={\"note\": \"replace with real RAG\"},\n",
    "        method_name=\"RAG\"\n",
    "    )\n",
    "\n",
    "def FT_ANSWER_FN(question: str) -> QAResult:\n",
    "    # TODO: call your real Fine-Tuned model here and return QAResult\n",
    "    return QAResult(\n",
    "        answer=\"PLACEHOLDER_FT_ANSWER\",\n",
    "        confidence=0.0,\n",
    "        metadata={\"note\": \"replace with real FT\"},\n",
    "        method_name=\"Fine-Tune\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e068d-2e72-46ab-af96-a354eaa11bb4",
   "metadata": {},
   "source": [
    "### 4.1 – Mandatory Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a5ea8-c3eb-4a52-8226-0656f87432bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.1: Mandatory Questions (3 items) ====\n",
    "# Categories must be: \"relevant_high_conf\", \"relevant_low_conf\", \"irrelevant\"\n",
    "\n",
    "mandatory_items = [\n",
    "    {\n",
    "        \"question\": \"[MANDATORY] (Relevant, high-confidence) What were Kyndryl’s total revenues in fiscal year 2024?\",\n",
    "        # $16,052 million per FY2024 report (≈$16.1B). Use numeric so the evaluator can apply tolerance.\n",
    "        \"ground_truth\": 16052000000,  # USD\n",
    "        \"category\": \"relevant_high_conf\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"[MANDATORY] (Relevant, low-confidence) What was the year-over-year (reported) revenue change for Kyndryl in fiscal year 2024 versus fiscal year 2023?\",\n",
    "        # Reported YoY decline shown as (6%) in FY2024 press/10-K tables.\n",
    "        \"ground_truth\": \"6% decline\",\n",
    "        \"category\": \"relevant_low_conf\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"[MANDATORY] (Irrelevant) What is the capital of France?\",\n",
    "        \"ground_truth\": \"Not in scope\",\n",
    "        \"category\": \"irrelevant\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e815b-f96f-4cdd-a734-bf30b8f8f51f",
   "metadata": {},
   "source": [
    "### 4.2 – Extended Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c05d2c-ef13-4743-aa6e-831b1fb0af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.2: Extended Evaluation Set with Accurate Values ====\n",
    "# Categories: \"financial_pnl\", \"financial_bs\", \"financial_cf\",\n",
    "# \"segment\", \"definition\", \"trend\", \"ratio\", \"other\"\n",
    "\n",
    "extended_items = [\n",
    "    # 1) Net loss FY2024: $340 million\n",
    "    {\n",
    "        \"question\": \"What was Kyndryl’s net loss for fiscal year 2024?\",\n",
    "        \"ground_truth\": -340000000,  # USD\n",
    "        \"category\": \"financial_pnl\",\n",
    "    },\n",
    "    # 2) Cash & cash equivalents end FY2024: $1,554 million\n",
    "    {\n",
    "        \"question\": \"How much cash and cash equivalents did Kyndryl report on its balance sheet at the end of fiscal year 2024?\",\n",
    "        \"ground_truth\": 1554000000,  # USD\n",
    "        \"category\": \"financial_bs\",\n",
    "    },\n",
    "    # 3) Net cash provided by operating activities FY2024: $454 million\n",
    "    {\n",
    "        \"question\": \"What was Kyndryl’s net cash provided by operating activities in fiscal year 2024?\",\n",
    "        \"ground_truth\": 454000000,  # USD\n",
    "        \"category\": \"financial_cf\",\n",
    "    },\n",
    "    # 4) FY2024 Adjusted EBITDA: $2.4 billion\n",
    "    {\n",
    "        \"question\": \"What was Kyndryl’s adjusted EBITDA for fiscal year 2024?\",\n",
    "        \"ground_truth\": 2400000000,  # USD\n",
    "        \"category\": \"financial_pnl\",\n",
    "    },\n",
    "    # 5) FY2023 cash flows from operations: $781 million (contrast)\n",
    "    {\n",
    "        \"question\": \"What were the cash flows from operations for Kyndryl in fiscal year 2023?\",\n",
    "        \"ground_truth\": 781000000,  # USD\n",
    "        \"category\": \"financial_cf\",\n",
    "    },\n",
    "    # 6) FY2023 net loss: $1.4 billion\n",
    "    {\n",
    "        \"question\": \"What was Kyndryl’s net loss for fiscal year 2023?\",\n",
    "        \"ground_truth\": -1400000000,  # USD\n",
    "        \"category\": \"financial_pnl\",\n",
    "    },\n",
    "    # 7) YoY revenue trend FY2024 vs FY2023: 6% decline\n",
    "    {\n",
    "        \"question\": \"What was the year-over-year revenue change (decline) in fiscal 2024 compared to fiscal 2023 for Kyndryl?\",\n",
    "        \"ground_truth\": \"6% decline\",\n",
    "        \"category\": \"trend\",\n",
    "    },\n",
    "    # 8) Signings FY2024: $12.5 billion\n",
    "    {\n",
    "        \"question\": \"How much were Kyndryl’s signings during fiscal year 2024?\",\n",
    "        \"ground_truth\": 12500000000,  # USD\n",
    "        \"category\": \"other\",\n",
    "    },\n",
    "    # 9) Alliances initiative revenue FY2024: >$500 million vs FY2023 $1.2B\n",
    "    {\n",
    "        \"question\": \"How much revenue did Kyndryl recognize from alliances in fiscal year 2024 versus fiscal year 2023?\",\n",
    "        \"ground_truth\": \"More than $500 million in 2024 and $1.2 billion in 2023\",\n",
    "        \"category\": \"segment\",\n",
    "    },\n",
    "    # 10) Adjusted free cash flow FY2024: $291 million\n",
    "    {\n",
    "        \"question\": \"What was Kyndryl’s adjusted free cash flow in fiscal year 2024?\",\n",
    "        \"ground_truth\": 291000000,  # USD\n",
    "        \"category\": \"financial_cf\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Export full questions template to CSV\n",
    "questions_df = pd.DataFrame(mandatory_items + extended_items)\n",
    "TEMPLATE_PATH = Path(\"section4_questions_template.csv\")\n",
    "questions_df.to_csv(TEMPLATE_PATH, index=False)\n",
    "print(f\"Updated template saved to: {TEMPLATE_PATH.resolve()}\")\n",
    "questions_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5178fd4-ffd9-4b87-ae05-531907bc9fd2",
   "metadata": {},
   "source": [
    "### Comparison Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15169445-bef1-43d8-9f6d-45822a329f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.x: Normalization, Matching, Out-of-Scope detection ====\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _extract_first_number(s: str) -> Optional[float]:\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).replace(\",\", \"\")\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def is_out_of_scope(answer: str) -> bool:\n",
    "    if answer is None:\n",
    "        return False\n",
    "    a = _normalize_text(answer)\n",
    "    phrases = [\n",
    "        \"not in scope\", \"out of scope\", \"no relevant data\",\n",
    "        \"cannot answer from the provided documents\",\n",
    "        \"not found in the documents\", \"data not available\",\n",
    "        \"i don't know\", \"i do not know\"\n",
    "    ]\n",
    "    return any(p in a for p in phrases)\n",
    "\n",
    "def compare_answers(ground_truth: Any, pred_text: str, rel_tol: float = 0.02) -> bool:\n",
    "    \"\"\"\n",
    "    Numeric: match within relative tolerance (default 2%).\n",
    "    Text: normalized exact/substring match.\n",
    "    'Not in scope' is checked via heuristic.\n",
    "    \"\"\"\n",
    "    if ground_truth is None:\n",
    "        return False\n",
    "\n",
    "    # numeric path\n",
    "    gold_num = ground_truth if isinstance(ground_truth, (int, float)) else _extract_first_number(ground_truth)\n",
    "    pred_num = _extract_first_number(pred_text)\n",
    "\n",
    "    if gold_num is not None and pred_num is not None:\n",
    "        if gold_num == 0:\n",
    "            return abs(pred_num) < 1e-12\n",
    "        return math.isclose(pred_num, float(gold_num), rel_tol=rel_tol)\n",
    "\n",
    "    # text path\n",
    "    g = _normalize_text(ground_truth)\n",
    "    p = _normalize_text(pred_text)\n",
    "\n",
    "    if g == \"not in scope\":\n",
    "        return is_out_of_scope(p)\n",
    "\n",
    "    return (g in p) or (p in g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8513b8-bb67-4ca6-b241-716265be26e8",
   "metadata": {},
   "source": [
    "### 4.3 – Per-Method Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a764a-b4f1-4293-8566-10f1fb417941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.3: Evaluate a single method on a list of Qs ====\n",
    "\n",
    "def evaluate_method(\n",
    "    questions: List[Dict[str, Any]],\n",
    "    method_fn: Callable[[str], QAResult],\n",
    "    method_label: str,\n",
    "    rel_tol: float = 0.02\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    logs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, row in enumerate(questions, start=1):\n",
    "        q = row[\"question\"]\n",
    "        gold = row[\"ground_truth\"]\n",
    "        category = row.get(\"category\", \"uncategorized\")\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            res = method_fn(q)\n",
    "        except Exception as e:\n",
    "            res = QAResult(answer=f\"[ERROR: {e}]\", confidence=None, metadata={\"exception\": str(e)}, method_name=method_label)\n",
    "        dt = time.perf_counter() - t0\n",
    "\n",
    "        correct = compare_answers(gold, res.answer, rel_tol=rel_tol)\n",
    "\n",
    "        logs.append({\n",
    "            \"id\": i,\n",
    "            \"question\": q,\n",
    "            \"ground_truth\": gold,\n",
    "            \"pred_answer\": res.answer,\n",
    "            \"confidence\": res.confidence if res.confidence is not None else np.nan,\n",
    "            \"time_s\": round(dt, 3),\n",
    "            \"correct\": bool(correct),\n",
    "            \"category\": category,\n",
    "            \"method\": method_label,\n",
    "            \"metadata_json\": json.dumps(res.metadata or {}, ensure_ascii=False),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a0c2c-fcbc-44be-80a5-dfb2d55f5c1f",
   "metadata": {},
   "source": [
    "### 4.4 – Full Comparison: RAG vs FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e727bd-3566-46a2-a623-a5a0803a284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.4: Run RAG vs FT and summarize ====\n",
    "\n",
    "ALL_QUESTIONS: List[Dict[str, Any]] = (mandatory_items + extended_items)\n",
    "\n",
    "rag_df = evaluate_method(ALL_QUESTIONS, RAG_ANSWER_FN, \"RAG\", rel_tol=0.02)\n",
    "ft_df  = evaluate_method(ALL_QUESTIONS, FT_ANSWER_FN,  \"Fine-Tune\", rel_tol=0.02)\n",
    "results_df = pd.concat([rag_df, ft_df], ignore_index=True)\n",
    "\n",
    "summary_df = (\n",
    "    results_df\n",
    "    .groupby(\"method\", as_index=False)\n",
    "    .agg(\n",
    "        accuracy=(\"correct\", \"mean\"),\n",
    "        avg_confidence=(\"confidence\", \"mean\"),\n",
    "        avg_time_s=(\"time_s\", \"mean\"),\n",
    "        n=(\"id\", \"count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(results_df.head(5))\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9a9a1-df88-4f1f-9fc4-903ce5e340ad",
   "metadata": {},
   "source": [
    "### 4.5 – Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce734ce-d08a-4c1f-a3aa-4ebfb7dd2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.5: Persist results to CSVs ====\n",
    "\n",
    "OUT_DIR = Path(\".\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESULTS_CSV = OUT_DIR / \"section4_results.csv\"\n",
    "SUMMARY_CSV = OUT_DIR / \"section4_summary.csv\"\n",
    "\n",
    "results_df.to_csv(RESULTS_CSV, index=False)\n",
    "summary_df.to_csv(SUMMARY_CSV, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(f\"- {RESULTS_CSV.resolve()}\")\n",
    "print(f\"- {SUMMARY_CSV.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f291c-997e-43b2-989f-d7dd6cb9e886",
   "metadata": {},
   "source": [
    "### 4.6 – Required Screenshots Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e4b40-fdb4-4f13-8237-a1e63062fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.6: Quick “screenshots” table filters ====\n",
    "# Use these to capture the 3 mandatory queries for your PDF.\n",
    "\n",
    "def show_mandatory(results: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Filter by the question text markers you set in 4.1\n",
    "    mask = results[\"question\"].str.contains(r\"\\[MANDATORY\\]\", regex=True, na=False)\n",
    "    return results[mask].copy()\n",
    "\n",
    "display(show_mandatory(results_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d128029-af65-49eb-afe1-1899a82dc8c7",
   "metadata": {},
   "source": [
    "### 4.7 – Category-wise Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b710f-d481-42cd-b4f6-766737fd5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.7: Extra analysis by category (optional but useful) ====\n",
    "\n",
    "by_cat = (\n",
    "    results_df\n",
    "    .groupby([\"method\", \"category\"], as_index=False)\n",
    "    .agg(\n",
    "        accuracy=(\"correct\", \"mean\"),\n",
    "        avg_time_s=(\"time_s\", \"mean\"),\n",
    "        n=(\"id\", \"count\")\n",
    "    )\n",
    "    .sort_values([\"method\", \"category\"])\n",
    ")\n",
    "\n",
    "display(by_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d0db4-dd95-4f21-bcc8-36104d207684",
   "metadata": {},
   "source": [
    "### 4.8 – Plots (Accuracy & Latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64659151-bf57-41c2-9134-bd3540f16543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Section 4.8: Simple Matplotlib charts (no styling/colors set) ====\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy\n",
    "plt.figure()\n",
    "plt.bar(summary_df[\"method\"], summary_df[\"accuracy\"])\n",
    "plt.title(\"Accuracy by Method\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Latency\n",
    "plt.figure()\n",
    "plt.bar(summary_df[\"method\"], summary_df[\"avg_time_s\"])\n",
    "plt.title(\"Average Response Time by Method (s)\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
