{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Group No : 43\n\n## Group Member Names:\n1. LAKSHMISRINIVAS PERAKAM\n2. SHAILESH KUMAR SINGH\n3. SUBHRANSU MISHRA\n4. JAWAHARLAL RAJAN S\n","metadata":{}},{"cell_type":"markdown","source":"1.**Problem statement**: \n\n* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n\n2.**Scenario**:\n* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\nGrid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected. \n\n\n#### Objective\n* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n\n#### About the environment\n\nThe environment consists of several types of tiles:\n* Start (S): The initial position of the agent, safe to step.\n* Frozen Tiles (F): Frozen surface, safe to step.\n* Hole (H): Falling into a hole ends the game immediately (die, end).\n* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game. \n\nAfter stepping on a treasure tile, it becomes a frozen tile (F).\nThe agent earns rewards as follows:\n* Reaching the goal (G): +10 reward.\n* Falling into a hole (H): -10 reward.\n* Collecting a treasure (T): +5 reward.\n* Stepping on a frozen tile (F): 0 reward.\n\n#### States\n* Current position of the agent (row, column).\n* A boolean flag (or equivalent) for whether each treasure has been collected.\n\n#### Actions\n* Four possible moves: up, down, left, right\n\n#### Rewards\n* Goal (G): +10.\n* Treasure (T): +5 per treasure.\n* Hole (H): -10.\n* Frozen tiles (F): 0.\n\n#### Environment\nModify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\nExample grid:\n\n![image.png](attachment:image.png)\n","metadata":{},"attachments":{"image.png":{"image/png":"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"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Expected Outcomes:**\n1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n4.\tVisualize the agent’s direction on the map using the learned policy.\n5.\tCalculate expected total reward over multiple episodes to evaluate performance.","metadata":{}},{"cell_type":"markdown","source":"### Import required libraries and Define the custom environment - 2 Marks","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport gym\nfrom gym.envs.toy_text.frozen_lake import FrozenLakeEnv\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:02:49.469261Z","iopub.execute_input":"2025-01-04T02:02:49.469743Z","iopub.status.idle":"2025-01-04T02:02:49.602381Z","shell.execute_reply.started":"2025-01-04T02:02:49.469680Z","shell.execute_reply":"2025-01-04T02:02:49.601076Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Custom environment to create the given grid and respective functions that are required for the problem\n\n#Include functions to take an action, get reward, to check if episode is over","metadata":{"execution":{"iopub.status.busy":"2024-12-27T12:20:49.604632Z","iopub.execute_input":"2024-12-27T12:20:49.604911Z","iopub.status.idle":"2024-12-27T12:20:49.611168Z","shell.execute_reply.started":"2024-12-27T12:20:49.604888Z","shell.execute_reply":"2024-12-27T12:20:49.610144Z"}}},{"cell_type":"code","source":"class FrozenLakeTreasureEnv(FrozenLakeEnv):\n    \"\"\"\n    Custom FrozenLake environment with treasures (T).\n    Inherits from OpenAI Gym's FrozenLakeEnv.\n    \"\"\"\n    def __init__(self, desc=None, is_slippery=False):\n        \"\"\"\n        Initializes the environment with a custom grid.\n        \n        Args:\n        - desc: Custom description of the grid (list of strings).\n        - is_slippery: If True, makes the environment slippery.\n        \"\"\"\n        if desc is None:\n            raise ValueError(\"A custom grid (desc) must be provided for the environment.\")\n        super().__init__(desc=desc, map_name=None, is_slippery=is_slippery)\n        self.treasure_positions = [(0, 4), (2, 3), (3, 0)]  # Example treasure locations\n        self.collected_treasures = set()  # To track collected treasures\n    def reset(self):\n        \"\"\"\n        Resets the environment to its initial state.\n        Clears the list of collected treasures.\n        \n        Returns:\n        - The initial state.\n        \"\"\"\n        self.collected_treasures = set()  # Reset collected treasures\n        return super().reset()\n\n    def step(self, action):\n        \"\"\"\n        Executes an action in the environment and returns the result.\n        Adds +5 reward for collecting treasures.\n        \"\"\"\n        # Call the parent class's step method\n        result = super().step(action)\n    \n        # Unpack the returned values appropriately\n        if len(result) == 5:\n            next_state, reward, done, info, extra = result\n            # If there's an extra value, you can ignore it or process it based on your needs\n        elif len(result) == 4:\n            next_state, reward, done, info = result\n        elif len(result) == 3:\n            next_state, reward, done = result\n            info = {}  # Default to an empty dictionary if `info` is not returned\n        else:\n            raise ValueError(f\"Unexpected number of return values from step: {len(result)}\")\n    \n        # Calculate the current position of the agent\n        row, col = self.s // self.ncol, self.s % self.ncol  # Convert state index to grid coordinates\n    \n        # Check if the agent has stepped on a treasure\n        if (row, col) in self.treasure_positions and (row, col) not in self.collected_treasures:\n            reward += 5  # Add reward for collecting a treasure\n            self.collected_treasures.add((row, col))  # Mark treasure as collected\n    \n        # Check if the episode ends (goal or hole)\n        if self.desc[row, col] in [b'G', b'H']:\n            done = True\n    \n        return next_state, reward, done, info\n\n\n    def is_episode_over(self):\n        \"\"\"\n        Checks if the episode has ended.\n        \n        Returns:\n        - True if the episode has ended, False otherwise.\n        \"\"\"\n        row, col = self.s // self.ncol, self.s % self.ncol  # Current position\n        return self.desc[row][col] in [b'G', b'H']  # End if at Goal or Hole\n\n    def render_custom(self):\n        \"\"\"\n        Renders the environment with additional info about collected treasures.\n        \"\"\"\n        print(\"Environment Grid:\")\n        self.render()\n        print(f\"Collected Treasures: {len(self.collected_treasures)} / {len(self.treasure_positions)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:02:49.603768Z","iopub.execute_input":"2025-01-04T02:02:49.604048Z","iopub.status.idle":"2025-01-04T02:02:49.615436Z","shell.execute_reply.started":"2025-01-04T02:02:49.604024Z","shell.execute_reply":"2025-01-04T02:02:49.614322Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Value Iteration Algorithm - 1 Mark","metadata":{}},{"cell_type":"code","source":"def value_iteration(env, gamma=0.9, theta=1e-4):\n    \"\"\"\n    Performs value iteration to compute the optimal value function (V*) and policy (π*).\n    \n    Parameters:\n    - env: The environment (FrozenLakeTreasureEnv)\n    - gamma: Discount factor\n    - theta: Convergence threshold\n    \n    Returns:\n    - V: Optimal value function for all states\n    - policy: Optimal policy (actions for each state)\n    \"\"\"\n    V = np.zeros(env.observation_space.n)  # Initialize value function\n    policy = np.zeros(env.observation_space.n, dtype=int)  # Initialize policy\n\n    while True:\n        delta = 0  # Tracks the maximum change in the value function\n        for s in range(env.observation_space.n):\n            # Determine if the state is terminal\n            row, col = s // env.ncol, s % env.ncol\n            if env.desc[row, col] in [b'H', b'G']:  # Hole or Goal\n                continue\n\n            # Compute Q-values for all actions\n            q_values = [\n                sum(p * (r + gamma * V[s_]) for p, s_, r, done in env.P[s][a])\n                for a in range(env.action_space.n)\n            ]\n            # Update the value function for state s\n            new_value = max(q_values)\n            delta = max(delta, abs(new_value - V[s]))\n            V[s] = new_value\n            # Update the policy to the action with the highest Q-value\n            policy[s] = np.argmax(q_values)\n\n        # Break if the value function has converged\n        if delta < theta:\n            break\n\n    return V, policy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:02:49.617507Z","iopub.execute_input":"2025-01-04T02:02:49.617863Z","iopub.status.idle":"2025-01-04T02:02:49.638210Z","shell.execute_reply.started":"2025-01-04T02:02:49.617821Z","shell.execute_reply":"2025-01-04T02:02:49.637301Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Policy Improvement Function - 1 Mark","metadata":{}},{"cell_type":"code","source":"def policy_improvement(env, V, gamma=0.9):\n    \"\"\"\n    Derives an improved policy based on the given value function (V).\n    \n    Parameters:\n    - env: The environment (FrozenLakeTreasureEnv)\n    - V: Current value function\n    - gamma: Discount factor\n    \n    Returns:\n    - policy: Improved policy\n    \"\"\"\n    policy = np.zeros(env.observation_space.n, dtype=int)\n    for s in range(env.observation_space.n):\n        if s in env.terminal_states:  # Skip terminal states\n            continue\n        # Compute Q values for all actions\n        q_values = [\n            sum(p * (r + gamma * V[s_]) for p, s_, r, done in env.P[s][a]) \n            for a in range(env.action_space.n)\n        ]\n        policy[s] = np.argmax(q_values)  # Choose the best action\n    return policy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:02:49.639337Z","iopub.execute_input":"2025-01-04T02:02:49.639614Z","iopub.status.idle":"2025-01-04T02:02:49.661323Z","shell.execute_reply.started":"2025-01-04T02:02:49.639589Z","shell.execute_reply":"2025-01-04T02:02:49.660253Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Print the Optimal Value Function","metadata":{}},{"cell_type":"code","source":"def print_value_function(V, env):\n    \"\"\"\n    Displays the optimal value function in grid form.\n    \"\"\"\n    grid = np.array(V).reshape(env.nrow, env.ncol)\n    print(\"Optimal Value Function:\")\n    print(grid)\n\ncustom_desc = [\n    \"SFFHT\",\n    \"FHFFF\",\n    \"FFFTF\",\n    \"TFHFF\",\n    \"FFFFG\"\n]\n\n# Assuming env is your environment and V is the optimal value function computed earlier\nenv = FrozenLakeTreasureEnv(desc=custom_desc, is_slippery=False)\n\n# Compute the optimal value function and policy using value iteration\nV, policy = value_iteration(env)\n\n# Print the optimal value function\nprint_value_function(V, env)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:02:49.662408Z","iopub.execute_input":"2025-01-04T02:02:49.662828Z","iopub.status.idle":"2025-01-04T02:02:49.692051Z","shell.execute_reply.started":"2025-01-04T02:02:49.662795Z","shell.execute_reply":"2025-01-04T02:02:49.691007Z"}},"outputs":[{"name":"stdout","text":"Optimal Value Function:\n[[0.4782969 0.531441  0.59049   0.        0.729    ]\n [0.531441  0.        0.6561    0.729     0.81     ]\n [0.59049   0.6561    0.729     0.81      0.9      ]\n [0.6561    0.729     0.        0.9       1.       ]\n [0.729     0.81      0.9       1.        0.       ]]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Visualization of the learned optimal policy - 1 Mark","metadata":{}},{"cell_type":"code","source":"def visualize_policy(env, policy):\n    \"\"\"\n    Visualizes the optimal policy as arrows on the grid.\n    \"\"\"\n    action_symbols = ['↑', '↓', '←', '→']  # Corresponding to actions 0, 1, 2, 3\n    grid = np.array(env.desc, dtype=str)\n    policy_grid = grid.copy()\n\n    for s in range(env.observation_space.n):\n        row, col = s // env.ncol, s % env.ncol\n        if grid[row, col] in ['H', 'G']:\n            continue\n        policy_grid[row, col] = action_symbols[policy[s]]\n\n    print(\"Learned Optimal Policy:\")\n    for row in policy_grid:\n        print(' '.join(row))\n\ncustom_desc = [\n    \"SFFHT\",\n    \"FHFFF\",\n    \"FFFTF\",\n    \"TFHFF\",\n    \"FFFFG\"\n]\n\n# Updated environment initialization\nenv = FrozenLakeTreasureEnv(desc=custom_desc, is_slippery=False)\n\n# Compute the optimal value function and policy using value iteration\nV, policy = value_iteration(env)\n\n# Visualize the learned optimal policy\nvisualize_policy(env, policy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:02:49.693073Z","iopub.execute_input":"2025-01-04T02:02:49.693341Z","iopub.status.idle":"2025-01-04T02:02:49.725019Z","shell.execute_reply.started":"2025-01-04T02:02:49.693318Z","shell.execute_reply":"2025-01-04T02:02:49.723782Z"}},"outputs":[{"name":"stdout","text":"Learned Optimal Policy:\n↓ ← ↓ H ↓\n↓ H ↓ ↓ ↓\n↓ ↓ ← ↓ ↓\n↓ ↓ H ↓ ↓\n← ← ← ← G\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Evaluate the policy - 1 Mark","metadata":{}},{"cell_type":"code","source":"def evaluate_policy(env, policy, num_episodes=100):\n    \"\"\"\n    Evaluates the given policy by running it over multiple episodes.\n    \n    Parameters:\n    - env: The environment\n    - policy: The policy to evaluate\n    - num_episodes: Number of episodes to run\n    \n    Returns:\n    - mean_reward: Average total reward over all episodes\n    - rewards: List of rewards for each episode\n    \"\"\"\n    rewards = []\n    for _ in range(num_episodes):\n        state = env.reset()\n        done = False\n        total_reward = 0\n\n        while not done:\n            action = policy[state]\n            state, reward, done, _ = env.step(action)\n            total_reward += reward\n\n        rewards.append(total_reward)\n    mean_reward = np.mean(rewards)\n    return mean_reward, rewards","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:02:49.726198Z","iopub.execute_input":"2025-01-04T02:02:49.726557Z","iopub.status.idle":"2025-01-04T02:02:49.747556Z","shell.execute_reply.started":"2025-01-04T02:02:49.726517Z","shell.execute_reply":"2025-01-04T02:02:49.746329Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Main Execution","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Define a custom 5x5 grid\n    custom_desc = [\n        \"SFFHT\",\n        \"FHFFF\",\n        \"FFFTF\",\n        \"TFHFF\",\n        \"FFFFG\"\n    ]\n\n    # Initialize the custom environment with the custom grid\n    env = FrozenLakeTreasureEnv(desc=custom_desc, is_slippery=False)\n\n    # Perform value iteration\n    V, policy = value_iteration(env)\n\n    # Print the optimal value function\n    print_value_function(V, env)\n\n    # Visualize the learned policy\n    visualize_policy(env, policy)\n\n    # Evaluate the policy\n    mean_reward, rewards = evaluate_policy(env, policy)\n    print(f\"Mean Reward over {len(rewards)} episodes: {mean_reward}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T02:02:49.749831Z","iopub.execute_input":"2025-01-04T02:02:49.750250Z","iopub.status.idle":"2025-01-04T02:02:49.787110Z","shell.execute_reply.started":"2025-01-04T02:02:49.750218Z","shell.execute_reply":"2025-01-04T02:02:49.786088Z"}},"outputs":[{"name":"stdout","text":"Optimal Value Function:\n[[0.4782969 0.531441  0.59049   0.        0.729    ]\n [0.531441  0.        0.6561    0.729     0.81     ]\n [0.59049   0.6561    0.729     0.81      0.9      ]\n [0.6561    0.729     0.        0.9       1.       ]\n [0.729     0.81      0.9       1.        0.       ]]\nLearned Optimal Policy:\n↓ ← ↓ H ↓\n↓ H ↓ ↓ ↓\n↓ ↓ ← ↓ ↓\n↓ ↓ H ↓ ↓\n← ← ← ← G\nMean Reward over 100 episodes: 6.0\n","output_type":"stream"}],"execution_count":8}]}